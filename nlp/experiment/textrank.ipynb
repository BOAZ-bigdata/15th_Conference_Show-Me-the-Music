{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"textrank.ipynb","private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyP43EG2h9Q3LsnZHTOJRXEp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"OHe-obJ8S_ji"},"outputs":[],"source":["from collections import Counter\n","\n","def scan_vocabulary(sents, tokenize, min_count=2):\n","    counter = Counter(w for sent in sents for w in tokenize(sent))\n","    counter = {w:c for w,c in counter.items() if c >= min_count}\n","    idx_to_vocab = [w for w, _ in sorted(counter.items(), key=lambda x:-x[1])]\n","    vocab_to_idx = {vocab:idx for idx, vocab in enumerate(idx_to_vocab)}\n","    return idx_to_vocab, vocab_to_idx"]},{"cell_type":"code","source":["from collections import defaultdict\n","\n","def cooccurrence(tokens, vocab_to_idx, window=2, min_cooccurrence=2):\n","    counter = defaultdict(int)\n","    for s, tokens_i in enumerate(tokens):\n","        vocabs = [vocab_to_idx[w] for w in tokens_i if w in vocab_to_idx]\n","        n = len(vocabs)\n","        for i, v in enumerate(vocabs):\n","            if window <= 0:\n","                b, e = 0, n\n","            else:\n","                b = max(0, i - window)\n","                e = min(i + window, n)\n","            for j in range(b, e):\n","                if i == j:\n","                    continue\n","                counter[(v, vocabs[j])] += 1\n","                counter[(vocabs[j], v)] += 1\n","    counter = {k:v for k,v in counter.items() if v >= min_cooccurrence}\n","    n_vocabs = len(vocab_to_idx)\n","    return dict_to_mat(counter, n_vocabs, n_vocabs)"],"metadata":{"id":"YVFZDvFgTInA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from scipy.sparse import csr_matrix\n","\n","def dict_to_mat(d, n_rows, n_cols):\n","    rows, cols, data = [], [], []\n","    for (i, j), v in d.items():\n","        rows.append(i)\n","        cols.append(j)\n","        data.append(v)\n","    return csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))"],"metadata":{"id":"diMnaC3dTMS_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def word_graph(sents, tokenize=None, min_count=2, window=2, min_cooccurrence=2):\n","    idx_to_vocab, vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)\n","    tokens = [tokenize(sent) for sent in sents]\n","    g = cooccurrence(tokens, vocab_to_idx, window, min_cooccurrence, verbose)\n","    return g, idx_to_vocab"],"metadata":{"id":"9h0OepCOTOSg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.preprocessing import normalize\n","\n","def pagerank(x, df=0.85, max_iter=30):\n","    assert 0 < df < 1\n","\n","    # initialize\n","    A = normalize(x, axis=0, norm='l1')\n","    R = np.ones(A.shape[0]).reshape(-1,1)\n","    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)\n","\n","    # iteration\n","    for _ in range(max_iter):\n","        R = df * (A * R) + bias\n","\n","    return R"],"metadata":{"id":"h-m7Vbo1TQMO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def textrank_keyword(sents, tokenize, min_count, window, min_cooccurrence, df=0.85, max_iter=30, topk=30):\n","    g, idx_to_vocab = word_graph(sents, tokenize, min_count, window, min_cooccurrence)\n","    R = pagerank(g, df, max_iter).reshape(-1)\n","    idxs = R.argsort()[-topk:]\n","    keywords = [(idx_to_vocab[idx], R[idx]) for idx in reversed(idxs)]\n","    return keywords"],"metadata":{"id":"0L_TnnPITS1g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","from scipy.sparse import csr_matrix\n","import math\n","\n","def sent_graph(sents, tokenize, similarity, min_count=2, min_sim=0.3):\n","    _, vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)\n","\n","    tokens = [[w for w in tokenize(sent) if w in vocab_to_idx] for sent in sents]\n","    rows, cols, data = [], [], []\n","    n_sents = len(tokens)\n","    for i, tokens_i in enumerate(tokens):\n","        for j, tokens_j in enumerate(tokens):\n","            if i >= j:\n","                continue\n","            sim = similarity(tokens_i, tokens_j)\n","            if sim < min_sim:\n","                continue\n","            rows.append(i)\n","            cols.append(j)\n","            data.append(sim)\n","    return csr_matrix((data, (rows, cols)), shape=(n_sents, n_sents))\n","\n","def textrank_sent_sim(s1, s2):\n","    n1 = len(s1)\n","    n2 = len(s2)\n","    if (n1 <= 1) or (n2 <= 1):\n","        return 0\n","    common = len(set(s1).intersection(set(s2)))\n","    base = math.log(n1) + math.log(n2)\n","    return common / base\n","\n","def cosine_sent_sim(s1, s2):\n","    if (not s1) or (not s2):\n","        return 0\n","\n","    s1 = Counter(s1)\n","    s2 = Counter(s2)\n","    norm1 = math.sqrt(sum(v ** 2 for v in s1.values()))\n","    norm2 = math.sqrt(sum(v ** 2 for v in s2.values()))\n","    prod = 0\n","    for k, v in s1.items():\n","        prod += v * s2.get(k, 0)\n","    return prod / (norm1 * norm2)"],"metadata":{"id":"-KIrK79KTUKc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def textrank_keysentence(sents, tokenize, min_count, similarity, df=0.85, max_iter=30, topk=5, min_sim=0.5):\n","    g = sent_graph(sents, tokenize, min_count, min_sim, similarity)\n","    R = pagerank(g, df, max_iter).reshape(-1)\n","    idxs = R.argsort()[-topk:]\n","    keysents = [(idx, R[idx], sents[idx]) for idx in reversed(idxs)]\n","    return keysents"],"metadata":{"id":"sksVqODJTXi5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install konlpy"],"metadata":{"id":"lY_lfRnuTeA2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from konlpy.tag import Komoran\n","\n","komoran = Komoran()\n","def komoran_tokenize(sent):\n","    sent = sent.replace('\\n','').replace('\\t','').replace('\\r','')\n","    words = komoran.pos(sent, join=True)\n","    words = [w for w in words if ('/NN' in w or '/XR' in w or '/VA' in w or '/VV' in w)]\n","    return words"],"metadata":{"id":"VwJIr-_aTZhr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["keyword_extractor = textrank_keyword(\n","    tokenize = komoran_tokenize,\n","    window = -1,\n","    verbose = False\n",")\n","\n","keywords = keyword_extractor.summarize(sents, topk=30)"],"metadata":{"id":"_LdrRluCTjrt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summarizer = KeysentenceSummarizer(tokenize = komoran_tokenize, min_sim = 0.5)\n","keysents = summarizer.summarize(sents, topk=10)"],"metadata":{"id":"I_J4htqKTdK0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sent = text = \"오늘은 발표회 날이다. 긴장이 되었다. 아침에 피자을 먹어서 기분이 좋아졌다! 발표회가 시작되고 1~2학년이 합쳐서 오카리나 연주을 들었다. 너무 아름다웠다. 마치 병아리가 아장아장 걸어다니는거 같다.그리고 인상깊은게 마지막에 오징어 게임ost가 인상이 깊다. 왜냐하면 1~2학년이 직접 녹음하고 무궁화 꽃이 피었습니다라고 말하고 1~2학년 탈락!이라고 하고 그리고 오징어 게임의 한장면을 연기한게 인상이 깊었다.단체로 쓰러지는게 쉽지는 않을텐데 1~2학년의 협동심이 대단한거 같다. 피아노 바와후 애들은 피아노을 발표하는데 하나하나 너무 아름다웠다. 마치 하나의 벛꽃같았다. 특히 울면안돼 플라워 댄스가 너무 인상깊었다. 3~4학년 아쉽게도 나한텐 인상이 안깊다. 노력은 했지만 좀 아쉽다. 근데 5학년는 내가 진짜 칭찬하고 싶은게 노력을 많이한게 보인다. 왜냐하면 아침시간, 점심시간, 중간놀이 등 연습을 했다. 5학년의 리코더 발표는 마치 참 참새의 소리가 같다. 그리고 댄스배틀이 나한테 가장인상이 깊었다. 왜냐하면 그냥 흥이 나고 재밌고애들이 웃어줘서 힘이나  춤추다가 안경 떨어지고 넘어지고 목도 다쳤지만 난 뿌듯하다. 그리고 6학년은 대단하다.  우리반이 너무 자랑스럽다. 왜냐하면 리코더을 너무 잘하고 실러폰도 잘했다. 노래가 마치 산비둘기 단체 구구구구 하는거 같이 잘 맞춰가고 흥이 났다. 우리가 발표한 연주는 똥 밟았네, 할아버지 시계, 젓가락 행진곡을 연주 했다. 나는 뿌듯하면서 행복했다. 이런 친구들이 있어서 너무 든든했다. 이런 친구들이 있었기에 내가 연주을 잘할수 있었던 것이다. 마지막에 합창을 했는데 마치 밤하늘의 별 같았다. 너무 좋고 아름답고 음도 좋다. 합창을 하는데 팔로 하트하는게 뿌끄러웠지만 용기내서 했다. 그치만 하고 이발표회는 끝이 났다. 내 자신이 자랑스럽고 뿌듯한 날이고 이번 오늘 아니 이날은 Good 아니 Great 아니 Nice 아니 이 날은 가치로 매길수 없는 날이다\""],"metadata":{"id":"Lstg8kFgUTVx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sent = sent.replace('\\n','').replace('\\t','').replace('\\r','')"],"metadata":{"id":"lAjyEiVxV74B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sent"],"metadata":{"id":"aQoVE0FUV_9d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from konlpy.tag import Kkma\n","\n","kkma = Kkma()\n","def kkma_tokenize(sent):\n","    #sent = sent.replace('\\n','').replace('\\t','').replace('\\r','')\n","    words = kkma.pos(sent, join=True)\n","    print(words)\n","    words = [w for w in words if ('/NN' in w or '/XR' in w or '/VA' in w or '/VV' in w)]\n","    return words"],"metadata":{"id":"STqnI8gLWbAm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def subword_tokenizer(sent, n=3):\n","    def subword(token, n):\n","        if len(token) <= n:\n","            return [token]\n","        return [token[i:i+n] for i in range(len(token) - n)]\n","    return [sub for token in sent.split() for sub in subword(token, n)]"],"metadata":{"id":"ONlgU7zCXUEv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["textrank_keysentence(sent, subword_tokenizer, min_count=2, similarity=textrank_sent_sim)"],"metadata":{"id":"FQsP3TOTUHWv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from gensim.summarization.summarizer import summarize"],"metadata":{"id":"Ba95PtVoUgMC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summarize(sent, ratio=0.05)"],"metadata":{"id":"g-8H2DIQX5-0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"cg33zBOiX92b"},"execution_count":null,"outputs":[]}]}